```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE,
                      comment = "#>",
                      fig.width = 16,
                      fig.height = 16,
                      fig.align = "center",
                      dev = "png",
                      cache = TRUE)
```

This tutorial is part of the Tools for Polyploid Training Workshop being held on March 17, 2023 both virtually and in person, in Nelson (New Zealand).


# What you will need to run this tutorial

* [Check here the requirements](https://cristianetaniguti.github.io/RGC11/workshop_requirements.html)

**warning**: Before start, it is important to highlight that all software here presented have several parameters that can be adjusted for each given scenario. Here, we will use the default settings, but for the best usage of each one of them, it is important to explore their specificity. So... use it carefully with other data sets!

## Useful Linux commands

```{bash, eval=FALSE}
getwd  # returns current directory path
ls     # list files in the current directory
cd     # change current directory
mv     # move/rename file or directory
mkdir  # create new directory
wget   # download file from link
echo   # print variable
less   # one possibility to read file

# Docker
docker pull                               # Get image from a registry
docker run                                # run a command in a new container
docker run -it -v                         # run in a interactive mode and transferring directory to container environment
docker images                             # list of images available
docker ps                                 # list containers
docker stop <cointainer_id>               # stop container
docker rm  <cointainer_id>                # remove container
docker rmi <image_id>                     # remove image
docker rmi $(docker images -q)            # remove all images
docker stop $(docker ps -a -q)            # stop all running containers 
docker rm $(docker ps -a -q)              # remove all stopped containers
docker build -t <image_name> Dockerfile   # build image from a Dockerfile
docker push                               # send the built image to a repository (Docker hub)
docker system prune                       # remove all unused containers, networks, images
```

## Getting the example data set for SNP calling

```{bash, eval=FALSE}
mkdir Tools2023
cd Tools2023

wget https://github.com/Cristianetaniguti/Reads2Map/raw/main/tests/data/polyploid/fastq/1.fastq.gz
wget https://github.com/Cristianetaniguti/Reads2Map/raw/main/tests/data/polyploid/fastq/98.fastq.gz
wget https://github.com/Cristianetaniguti/Reads2Map/raw/main/tests/data/polyploid/fastq/P1.fastq.gz
wget https://github.com/Cristianetaniguti/Reads2Map/raw/main/tests/data/polyploid/fastq/P2.fastq.gz

wget https://github.com/Cristianetaniguti/Reads2Map/raw/develop/tests/data/polyploid/RchinensisV1.0/Chr04_sub.fasta
```

# Sequences Quality Control

A classic way to have a overview of our FASTQ sequences is the FASTQC software:

```{bash, eval=FALSE}
for i in *.fastq.gz; do # Will run in all files in the directory finishing with fastq
    echo $i
    docker run -v $(pwd):/opt biocontainers/fastqc:v0.11.9_cv7 fastqc /opt/$i
done
```

or

```{bash, eval=FALSE}
for i in *.fastq.gz; do # Will run in all files in the directory finishing with fastq
    echo $i
    singularity exec --bind $(pwd):/opt $SCRATCH/.singularity/biocontainers_fastqc_v0.11.9_cv7.sif fastqc /opt/$i
done
```

See [here](https://cristianetaniguti.github.io/tools2023/1_fastqc.html) the result for sample 1.

It returns to us an HTML file with several precious pieces of information. 
Instead of checking separated HTML by sample, we can join them using the very fancy MULTIQC tool. 
This profile is typical from GBS sequences, other types of libraries would return other profiles and could have different problems. 

Now, that we already checked how our sequences are, we can decide which filters to use to clean them. Here, the samples are already filtered and demultiplexed, but if they are not, process_radtags from STACKs perform the demultiplexing, removal of adaptor sequences, and some other quality filters, as searching for the enzyme cut site and according to sequence Phred scale. Check its manual [here](https://catchenlab.life.illinois.edu/stacks/comp/process_radtags.php). 

# Reads2Map workflows

[Reads2Map](https://github.com/Cristianetaniguti/Reads2Map) is a collection of WDL workflows to build linkage maps from sequencing data. It was first developed for diploids ([preprint](https://www.biorxiv.org/content/10.1101/2022.11.24.517847v2)) and is being expanded for polyploids. 

You can get more information about each subworkflow, input files and parameters in the [EmpiricalReads2Map vignette](https://cristianetaniguti.github.io/Tutorials/Reads2Map/EmpiricalReads.html).

## SNP calling with GATK (EmpiricalSNPCalling.wdl)

We will use the EmpiricalSNPCalling.wdl to perform:

* Alignment with BWA (parallelize the samples in cores and nodes)
* SNP calling with GATK (parallelize the samples in cores and nodes)
* [GATK hard filtering](https://gatk.broadinstitute.org/hc/en-us/articles/360035890471-Hard-filtering-germline-short-variants)

What we will need:

* [EmpiricalSNPCalling workflow - Reads2Map](https://github.com/Cristianetaniguti/Reads2Map/releases/tag/EmpiricalSNPCalling_v1.2.1)
* [Configuration files](https://github.com/Cristianetaniguti/Reads2Map/tree/main/.configurations)
* [Download cromwell](https://github.com/broadinstitute/cromwell/releases)
* Java

```{bash, eval=FALSE}
# cromwell (already available in the local server, use $CROMWELL_JAR)
# wget https://github.com/broadinstitute/cromwell/releases/download/85/cromwell-85.jar

wget https://github.com/Cristianetaniguti/Reads2Map/releases/download/EmpiricalSNPCalling_v1.2.1/EmpiricalSNPCalling_v1.2.1.wdl
wget https://github.com/Cristianetaniguti/Reads2Map/releases/download/EmpiricalSNPCalling_v1.2.1/EmpiricalSNPCalling_v1.2.1.zip
wget https://github.com/Cristianetaniguti/Reads2Map/raw/develop/.configurations/cromwell_no_mysql.conf
wget https://github.com/Cristianetaniguti/Reads2Map/raw/main/tests/pipelines/EmpiricalSNPCalling/EmpiricalSNPCalling_inputs.json
wget https://github.com/Cristianetaniguti/Reads2Map/raw/main/tests/pipelines/EmpiricalSNPCalling/samples_info.txt
```

Genome index files

```{bash, eval=FALSE}
docker run -v $(pwd):/opt/ kfdrc/bwa-picard:latest-dev bwa index /opt/Chr04_sub.fasta

docker run -v $(pwd):/opt/ kfdrc/bwa-picard:latest-dev  java -jar /picard.jar CreateSequenceDictionary \
    R=/opt/Chr04_sub.fasta \
    O=/opt/Chr04_sub.dict

docker run -v $(pwd):/opt/ cristaniguti/r-samtools:latest samtools faidx /opt/Chr04_sub.fasta
```


Input file `EmpiricalSNPCalling_inputs.json`:

```
{
  "SNPCalling.max_cores": 2,
  "SNPCalling.ploidy": 4,
  "SNPCalling.rm_dupli": false,
  "SNPCalling.run_gatk": true,
  "SNPCalling.run_freebayes": false,
  "SNPCalling.hardfilters": true,
  "SNPCalling.n_chrom": 1,
  "SNPCalling.chunk_size": 1,
  "SNPCalling.samples_info": "samples_info.txt",
  "SNPCalling.references": {
    "ref_fasta": "Chr04_sub.fasta",
    "ref_dict": "Chr04_sub.dict",
    "ref_ann": "Chr04_sub.fasta.ann",
    "ref_sa": "Chr04_sub.fasta.sa",
    "ref_amb": "Chr04_sub.fasta.amb",
    "ref_pac": "Chr04_sub.fasta.pac",
    "ref_bwt": "Chr04_sub.fasta.bwt",
    "ref_fasta_index": "Chr04_sub.fasta.fai"
  }
}
```

samples_info.txt:

```
1.fastq.gz   1   1
98.fastq.gz   98    98
P1.fastq.gz   P1    P1
P2.fastq.gz   P2    P2
```

If you are running using singularity combined with SLURM, set the default in `cromwell_no_mysql.conf` to `SlurmSingularity`, and run:

```{bash, eval=FALSE}
java -jar -Dconfig.file=cromwell_no_mysql.conf -jar cromwell-85.jar run EmpiricalSNPCalling_v1.2.1.wdl \
                      -i EmpiricalSNPCalling_inputs.json \
                      -p EmpiricalSNPCalling_v1.2.1.zip
```

If you are using docker, set the default in `cromwell_no_mysql.conf` to `Local`, and run:

```{bash, eval=FALSE}
java -jar -Dconfig.file=cromwell_no_mysql.conf -jar $CROMWELL_JAR run EmpiricalSNPCalling_v1.2.1.wdl \
                      -i EmpiricalSNPCalling_inputs.json \
                      -p EmpiricalSNPCalling_v1.2.1.zip
```

Check the results in the outputted path printed in the workflow log. 

```
  "outputs":
{
  "SNPCalling.vcfs_software": ["gatk"],
  "SNPCalling.gatk_multi_vcf": null,
  "SNPCalling.vcfs_counts_source": ["vcf"],
  "SNPCalling.Plots": "/home/chtaniguti/Tools2023/cromwell-executions/SNPCalling/417e96d2-d154-466e-9a50-0ee319c10c5e/call-GatkGenotyping/GatkGenotyping/1773aa18-7b48-4e3e-9aa4-4405b1a8276a/call-HardFilteringEmp/HardFilteringEmp/4e26f024-8d38-454e-b8c4-04054cedb9bd/call-QualPlots/execution/QualPlots.tar.gz",
  "SNPCalling.merged_bam": "/home/chtaniguti/Tools2023/cromwell-executions/SNPCalling/417e96d2-d154-466e-9a50-0ee319c10c5e/call-CreateAlignmentFromFamilies/CreateAlignmentFromFamilies/f6b9df2d-4007-4a2e-8db5-978429dcf390/call-MergeBams/execution/merged.bam",
  "SNPCalling.freebayes_vcfEval": null,
  "SNPCalling.gatk_vcfEval": "/home/chtaniguti/Tools2023/cromwell-executions/SNPCalling/417e96d2-d154-466e-9a50-0ee319c10c5e/call-GatkGenotyping/GatkGenotyping/1773aa18-7b48-4e3e-9aa4-4405b1a8276a/call-Normalization/Normalization/811b36f9-bc8f-451a-8556-21213fdc46cf/call-VariantEval/execution/vcfEval.txt",
  "SNPCalling.vcfs": ["/home/chtaniguti/Tools2023/cromwell-executions/SNPCalling/417e96d2-d154-466e-9a50-0ee319c10c5e/call-GatkGenotyping/GatkGenotyping/1773aa18-7b48-4e3e-9aa4-4405b1a8276a/call-Normalization/Normalization/811b36f9-bc8f-451a-8556-21213fdc46cf/call-BiallelicNormalization/execution/vcf_norm.vcf.gz"]
}
```

## Dosage calling and recombination fraction matrix (EmpiricalMaps.wdl)

We will use the EmpiricalMaps.wdl to perform:

* Dosage calling with updog, SuperMASSA and polyRAD
* Filtering markers by genotype probabilities, missing data, segregation distortion and redundancy using MAPpoly
* Estimate recombination fraction using MAPpoly
* Plot recombination fraction heatmaps using MAPpoly


Download the workflow and the example data set:

```{bash, eval=FALSE}
# Workflow
wget https://github.com/Cristianetaniguti/Reads2Map/releases/download/EmpiricalMaps_v1.2.3/EmpiricalMaps_v1.2.3.wdl
wget https://github.com/Cristianetaniguti/Reads2Map/releases/download/EmpiricalMaps_v1.2.3/EmpiricalMaps_v1.2.3.zip
wget https://github.com/Cristianetaniguti/Reads2Map/raw/main/tests/pipelines/EmpiricalMaps/polyploid/EmpiricalMaps_inputs.json

# Data set (62 rose individuals Chr04)
wget https://github.com/Cristianetaniguti/Reads2Map/raw/main/tests/data/polyploid/vcfs_norm/gatk_Chr04_filt_example.vcf.gz
```

Prepare EmpiricalMaps_inputs.json:

```
{
  "Maps.ploidy": "4",
  "Maps.dataset": {
    "parent2": "P1",
    "name": "roses",
    "parent1": "P2",
    "chromosome": "Chr04",
    "cross": "F1",
    "multiallelics": "false"
  },
  "Maps.max_cores": "2",
  "Maps.gatk_mchap": "false",
  "Maps.vcfs_counts_source": ["vcf"],
  "Maps.vcfs_software": ["gatk"],
  "Maps.filter_noninfo": "true",
  "Maps.vcfs": ["gatk_Chr04_filt_example.vcf.gz"],
  "Maps.replaceADbyMissing": "TRUE",
  "Maps.prob_thres": 0.8
}
```

```{bash, eval=FALSE}
java -jar -Dconfig.file=cromwell_no_mysql.conf -jar $CROMWELL_JAR run EmpiricalMaps_v1.2.3.wdl \
                        -i EmpiricalMaps_inputs.json \
                        -p EmpiricalMaps_v1.2.3.zip
```

Check the results in the outputted path printed in the workflow log.

```
  "outputs": {
    "Maps.EmpiricalReads_results": "/home/chtaniguti/Tools2023/cromwell-executions/Maps/865ad8bc-8158-4cff-8a98-874e2df75b20/call-JointReportsPoly/execution/EmpiricalReads_results_poly.tar.gz"
  },
```

# Step-by-step (not using workflows)

Here we run the SNP and dosage calling using [BWA](http://bio-bwa.sourceforge.net/bwa.shtml) and [GATK](https://gatk.broadinstitute.org/hc/en-us) just for a subset of a tetraploid rose GBS data set. This subset contains only four individuals and sequences that aligned to the half of *Rosa chinensis* chromosome 4. Check the RAM and CPU that was used to run the same pipeline for the full data set below:

![Computational resources required to run this pipeline in the rose complete data (mean depth ~83 and population size of 114 progeny)](../tools2023/images/log10_efficiency_rose.png)


## Alignment

There are many software to perform the alignment. The suggested by GATK team is the BWA-MEM.

BWA requires some index files before the alignment, you can build them with:

```{bash, eval=FALSE}
docker run -v $(pwd):/opt/ kfdrc/bwa-picard:latest-dev bwa index /opt/Chr04_sub.fasta

docker run -v $(pwd):/opt/ kfdrc/bwa-picard:latest-dev  java -jar /picard.jar CreateSequenceDictionary \
    R=/opt/Chr04_sub.fasta \
    O=/opt/Chr04_sub.dict
```

or

```{bash, eval=FALSE}
singularity run --bind $(pwd):/opt/ $SCRATCH/.singularity/kfdrc_bwa-picard_latest-dev.sif bwa index /opt/Chr04_sub.fasta

singularity run --bind $(pwd):/opt/ $SCRATCH/.singularity/kfdrc_bwa-picard_latest-dev.sif  java -jar /picard.jar  CreateSequenceDictionary \
    R=/opt/Chr04_sub.fasta \
    O=/opt/Chr04_sub.dict
```
  

Run BWA-MEM for each sample file:

```{bash, eval=FALSE}
for i in *.fastq.gz; do
  echo $i
  filename="${i%%.*}"
  echo $filename
  #Alignment
  docker run -v $(pwd):/opt/ kfdrc/bwa-picard:latest-dev bwa mem -t 2 \
    -R "@RG\tID:$filename\tLB:lib1\tPL:illumina\tSM:$filename\tPU:FLOWCELL1.LANE1" \
    /opt/Chr04_sub.fasta /opt/$i > $filename.bam

  #Sort BAM File
  docker run -v $(pwd):/opt kfdrc/bwa-picard:latest-dev java -jar /picard.jar SortSam \
    I="/opt/$filename.bam" \
    O="/opt/$filename.sorted.bam" \
    TMP_DIR=./tmp \
    SORT_ORDER=coordinate \
    CREATE_INDEX=true
done
```

or

```{bash, eval=FALSE}
for i in *fastq.gz; do
  echo $i
  filename="${i%%.*}"
  echo $filename
  #Alignment
  singularity run --bind $(pwd):/opt/ $SCRATCH/.singularity/kfdrc_bwa-picard_latest-dev.sif bwa mem -t 2 \
    -R "@RG\tID:$filename\tLB:lib1\tPL:illumina\tSM:$filename\tPU:FLOWCELL1.LANE1" \
    /opt/Chr04_sub.fasta /opt/$i > $filename.bam
    
  #Sort BAM File
  singularity run --bind $(pwd):/opt $SCRATCH/.singularity/kfdrc_bwa-picard_latest-dev.sif java -jar /picard.jar SortSam \
    I="/opt/$filename.bam" \
    O="/opt/$filename.sorted.bam" \
    TMP_DIR=./tmp \
    SORT_ORDER=coordinate \
    CREATE_INDEX=true
done
```


If you have more than one genome and want to test how much your sample aligns with each one, [FASTQ screen](https://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/) is a good tool for that.

## GATK - Joint Call

GATK is a toolkit with more than 200 available tools. You can imagine that a practical guide is not a place to explain all its features. Anyway, the Broad Institute team made a really good work with tutorials on their [website](https://gatk.broadinstitute.org/hc/en-us). GATK also has workflows available [here](https://gatk.broadinstitute.org/hc/en-us/sections/360007226651-Best-Practices-Workflows) for the most common scenarios. The bad news is that RADseq or GBS data sets are not included in these scenarios. Therefore, some adaptations are required. For example, the duplicates are commonly removed from data sets such as WGS and exome, but must be kept in GBS data. Other example is the type of filtering applied to the identified markers. If you have lots of previous information about your species, such as a good reference genome or a marker database, these information can be used to increase SNP and genotype calling in GATK with tools such as BQSR and Variant recalibrator. If you do not have much previous information (what is common in plant research), GATK provides several quality measures for each marker called and you can apply what they call ["hard filters"](https://gatk.broadinstitute.org/hc/en-us/articles/360035890471-Hard-filtering-germline-short-variants) in your markers.

The called Joint Call method run the variant calling on each sample and produces the g.vcf files (one by sample). The g.vcfs have records for every position in the genome. After, a database is created to join the individual samples information. The traditional VCF file (containing all samples) can be extracted from the database. The analysis is done separately by sample for several reasons, one is flexibility of processing, we can parallelize the way we want. Another is the called N+1 problem, which means that with the database created, you don't need to repeat all the analysis if you want to add an extra sample. 

```{bash, eval=FALSE}
# It requires other indexes
docker run -v $(pwd):/opt/ cristaniguti/r-samtools:latest samtools faidx /opt/Chr04_sub.fasta

for i in *sorted.bam; do
  filename="${i%%.*}"
  echo $filename
  docker run -v $(pwd):/opt taniguti/gatk-picard:latest /gatk/gatk HaplotypeCaller \
                                                    -ERC GVCF \
                                                    -R /opt/Chr04_sub.fasta \
                                                    -ploidy 4 \
                                                    -I /opt/$i \
                                                    -O /opt/$filename.g.vcf \
                                                    --max-reads-per-alignment-start 0
done

grep ">" Chr04_sub.fasta > interval_list_temp # Find scaffold name
sed 's/^.//' interval_list_temp > interval_list_temp2 # remove > at the beginning
awk  '{print $1;}' interval_list_temp2 > interval.list # gets only the first word

docker run -v $(pwd):/opt taniguti/gatk-picard:latest /gatk/gatk GenomicsDBImport \
                                                  --genomicsdb-workspace-path /opt/my_database \
                                                  -L /opt/interval.list \
                                                  -V /opt/1.g.vcf \
                                                  -V /opt/98.g.vcf \
                                                  -V /opt/P1.g.vcf \
                                                  -V /opt/P2.g.vcf 

docker run -v $(pwd):/opt taniguti/gatk-picard:latest /gatk/gatk GenotypeGVCFs \
                                                   -R /opt/Chr04_sub.fasta\
                                                   -O /opt/gatk.vcf.gz \
                                                   -G StandardAnnotation \
                                                   -V gendb:///opt/my_database
                                                   
```

or 

```{bash, eval=FALSE}
# It requires other indexes
singularity exec --bind $(pwd):/opt/ $SCRATCH/.singularity/cristaniguti_r-samtools_latest.sif samtools faidx /opt/Chr04_sub.fasta

for i in *sorted.bam; do
  filename="${i%%.*}"
  echo $filename
  singularity exec --bind $(pwd):/opt $SCRATCH/.singularity/taniguti_gatk-picard_latest.sif /gatk/gatk HaplotypeCaller \
                                                    -ERC GVCF \
                                                    -R /opt/Chr04_sub.fasta\
                                                    -ploidy 4 \
                                                    -I /opt/$i \
                                                    -O /opt/$filename.g.vcf \
                                                    --max-reads-per-alignment-start 0
done

grep ">" Chr04_sub.fasta > interval_list_temp # Find scaffold name
sed 's/^.//' interval_list_temp > interval_list_temp2 # remove > at the beginning
awk  '{print $1;}' interval_list_temp2 > interval.list # gets only the first word

singularity exec --bind $(pwd):/opt $SCRATCH/.singularity/taniguti_gatk-picard_latest.sif /gatk/gatk GenomicsDBImport \
                                                  --genomicsdb-workspace-path /opt/my_database \
                                                  -L /opt/interval.list \
                                                  -V /opt/1.g.vcf \
                                                  -V /opt/98.g.vcf \
                                                  -V /opt/P1.g.vcf \
                                                  -V /opt/P2.g.vcf 

singularity exec --bind $(pwd):/opt $SCRATCH/.singularity/taniguti_gatk-picard_latest.sif /gatk/gatk GenotypeGVCFs \
                                                   -R /opt/Chr04_sub.fasta \
                                                   -O /opt/gatk.vcf.gz \
                                                   -G StandardAnnotation \
                                                   -V gendb:///opt/my_database
                                                   
```

gatk.vcf.gz:

```
#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  1       98      P1      P2
Chr04   21571   .       G       T       3156.21 .       AC=12;AF=1.00;AN=12;DP=76;FS=0.000;MLEAC=13;MLEAF=1.00;MQ=60.00;QD=25.36;SOR=8.688      GT:AD:DP:GQ:PL  1/1/1/1:0,3:3:4:125,18,9,4,0    1/1/1/1:0,7:7:9:292,42,21,9,0   1/1/1/1:0,66:66:82:2724,396,198,82,0    ./././.:0,0:0:.:0,0,0,0,0
Chr04   68559   .       C       T       55.28   .       AC=1;AF=0.083;AN=12;BaseQRankSum=0.705;DP=104;FS=0.000;MLEAC=1;MLEAF=0.083;MQ=60.00;MQRankSum=0.00;QD=1.35;ReadPosRankSum=-5.340e-01;SOR=0.010  GT:AD:DP:GQ:PL  ./././.:0,0:0:.:0,0,0,0,0       0/0/0/1:38,3:41:58:60,0,58,167,1576     0/0/0/0:1,0:1:1:0,1,3,6,42      0/0/0/0:62,0:62:50:0,50,120,241,1800
Chr04   68581   .       C       T       818.57  .       AC=3;AF=0.250;AN=12;BaseQRankSum=-2.420e+00;DP=100;FS=0.000;MLEAC=4;MLEAF=0.333;MQ=60.00;MQRankSum=0.00;QD=8.27;ReadPosRankSum=1.80;SOR=0.211   GT:AD:DP:GQ:PL  ./././.:0,0:0:.:0,0,0,0,0       0/0/1/1:15,26:41:1:732,48,0,1,515       0/0/0/0:1,0:1:1:0,1,3,6,42      0/0/0/1:52,6:58:75:88,0,75,221,2055
Chr04   68593   .       C       T       47.28   .       AC=1;AF=0.083;AN=12;BaseQRankSum=-1.709e+00;DP=100;FS=0.000;MLEAC=1;MLEAF=0.083;MQ=60.00;MQRankSum=0.00;QD=0.82;ReadPosRankSum=3.02;SOR=0.008   GT:AD:DP:GQ:PL  ./././.:0,0:0:.:0,0,0,0,0       0/0/0/0:41,0:41:50:0,50,120,241,1800    0/0/0/0:1,0:1:1:0,1,3,6,30      0/0/0/1:54,4:58:52:52,0,83,239,2285
Chr04   68612   .       C       T       1512.17 .       AC=3;AF=0.250;AN=12;BaseQRankSum=0.775;DP=100;FS=0.000;MLEAC=4;MLEAF=0.333;MQ=60.00;MQRankSum=0.00;QD=15.27;ReadPosRankSum=7.39;SOR=0.358       GT:AD:DP:GQ:PL  ./././.:0,0:0:.:0,0,0,0,0       0/0/0/1:38,3:41:58:69,0,58,167,1644     0/0/0/0:1,0:1:1:0,1,3,6,30      0/0/1/1:22,36:58:3:1445,70,0,3,815
Chr04   68619   .       C       T       1512.17 .       AC=3;AF=0.250;AN=12;BaseQRankSum=0.554;DP=100;FS=0.000;MLEAC=4;MLEAF=0.333;MQ=60.00;MQRankSum=0.00;QD=15.27;ReadPosRankSum=7.39;SOR=0.358       GT:AD:DP:GQ:PL  ./././.:0,0:0:.:0,0,0,0,0       0/0/0/1:38,3:41:58:69,0,58,167,1644     0/0/0/0:1,0:1:1:0,1,3,6,30      0/0/1/1:22,36:58:3:1445,70,0,3,815
```

## Dosage calling

Install the R packages:

```{r, eval=FALSE}
install.packages("polyRAD")
install.packages("updog")
install.packages("fitPoly") 

install.packages("vcfR") # To manipulate VCF file inside R
```

Check their documentation:

* [polyRAD](https://github.com/lvclark/polyRAD)
* [updog](https://dcgerard.github.io/updog/)

Check this tutorial if you have array data:

*[Dosage scoring / fitPoly workshop](https://www.polyploids.org/sites/default/files/2021-01/SCRI_fitPoly_workshop.html)

Example considering a $F_1$ outcrossing population:

### Example with updog

Get the example data. It is the same data set as before, but now with 62 samples and a subset of chromosome 04.

```{bash, eval=FALSE}
wget https://github.com/Cristianetaniguti/Reads2Map/raw/main/tests/data/polyploid/vcfs_norm/gatk_Chr04_filt_example.vcf.gz
```

```{r, eval=FALSE}
library(vcfR)

vcf <- read.vcfR("gatk_Chr04_filt_example.vcf.gz")
vcf

sizemat <- extract.gt(vcf, "DP")

ADmat <- extract.gt(vcf, "AD")
refmat <- strsplit(ADmat, ",")
refmat <- sapply(refmat, "[[", 1)
refmat <- matrix(refmat, nrow = nrow(ADmat))
refmat[1:5, 1:5]
ADmat[1:5,1:5]

library(updog)

mout <- multidog(refmat = refmat, 
                 sizemat = sizemat, 
                 ploidy = 4, 
                 model = "f1",
                 nc = 2)

sizemat <- apply(sizemat, 2, as.numeric)
refmat <- apply(refmat, 2, as.numeric)

mout <- multidog(refmat = refmat, 
                 sizemat = sizemat, 
                 ploidy = 4, 
                 model = "f1",
                 nc = 2)

colnames(refmat) <- colnames(sizemat) <- colnames(ADmat)
rownames(refmat) <- rownames(sizemat) <- rownames(ADmat)

mout <- multidog(refmat = refmat, 
                 sizemat = sizemat, 
                 ploidy = 4, 
                 model = "f1",
                 nc = 2)

plot(mout, indices = c(1, 1, 10))

genomat <- format_multidog(mout, varname = "geno")
head(genomat)
```

